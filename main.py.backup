#!/usr/bin/env python3
"""
Pipeline principal pour le projet AI-MAP, inspirÃ© d'EMUT.
Orchestre l'ensemble du traitement des donnÃ©es gÃ©ophysiques et du pipeline d'entraÃ®nement CNN.
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import torch
import numpy as np

# Ajouter le rÃ©pertoire courant au path Python
sys.path.insert(0, str(Path(__file__).parent))

try:
    from config import CONFIG
    from src.utils.logger import logger
    
    # ============================================================================
    # PHASE 1: NETTOYAGE ET PRÃ‰TRAITEMENT DES DONNÃ‰ES
    # ============================================================================
    
    def phase1_data_cleaning() -> Dict[str, Any]:
        """
        Phase 1: Nettoyage et prÃ©traitement des donnÃ©es gÃ©ophysiques.
        
        Returns:
            Dict contenant les rÃ©sultats du nettoyage
        """
        logger.info("ğŸ“‹ Phase 1: Nettoyage et prÃ©traitement des donnÃ©es")
        logger.info("-" * 40)
        
        # Initialiser le nettoyeur de donnÃ©es
        from src.preprocessor.data_cleaner import GeophysicalDataCleaner
        cleaner = GeophysicalDataCleaner()
        
        # Nettoyer les donnÃ©es de tous les dispositifs
        cleaning_results = cleaner.clean_all_devices()
        
        logger.info("âœ… Nettoyage des donnÃ©es terminÃ© avec succÃ¨s")
        logger.info("Rapport de nettoyage:")
        for device_name, (clean_path, report) in cleaning_results.items():
            logger.info(f"  {device_name}: {report.get('cleaned_count', 0)}/{report.get('original_count', 0)} enregistrements conservÃ©s")
            
        return cleaning_results
    
    # ============================================================================
    # PHASE 2: TRAITEMENT DES DONNÃ‰ES ET CRÃ‰ATION DES GRILLES
    # ============================================================================
    
    def phase2_data_processing() -> Tuple[Optional[Any], Optional[np.ndarray], Optional[np.ndarray]]:
        """
        Phase 2: Traitement des donnÃ©es et crÃ©ation des grilles spatiales.
        
        Returns:
            Tuple contenant (processor, multi_device_tensor, volume_3d)
        """
        logger.info("\nğŸ“Š Phase 2: Traitement des donnÃ©es et crÃ©ation des grilles")
        logger.info("-" * 40)
        
        # Initialiser le processeur de donnÃ©es
        from src.data.data_processor import GeophysicalDataProcessor
        processor = GeophysicalDataProcessor()
        
        # Charger et valider les donnÃ©es nettoyÃ©es
        device_data = processor.load_and_validate()
        
        # Utiliser les donnÃ©es rÃ©elles si disponibles
        if not device_data:
            logger.warning("Aucune donnÃ©e de dispositif valide trouvÃ©e aprÃ¨s le nettoyage")
            logger.info("Le pipeline continuera avec des donnÃ©es factices pour la dÃ©monstration")
            
            # CrÃ©er des donnÃ©es factices pour la dÃ©monstration
            n_samples = 50
            n_channels = 4
            grid_size = 64
            
            # Tenseur multi-dispositifs factice
            multi_device_tensor = np.random.rand(n_samples, n_channels, grid_size, grid_size).astype(np.float32)
            
            # Volume 3D factice
            volume_3d = np.random.rand(20, n_channels, 32, 32, 32).astype(np.float32)
            
            logger.info("âœ… DonnÃ©es factices crÃ©Ã©es pour la dÃ©monstration")
            logger.info(f"Forme du tenseur multi-dispositifs: {multi_device_tensor.shape}")
            logger.info(f"Forme du volume 3D: {volume_3d.shape}")
            
            return processor, multi_device_tensor, volume_3d
    
        # CrÃ©er les grilles spatiales
        spatial_grids = processor.create_spatial_grids()
        
        # CrÃ©er le tenseur multi-dispositifs pour l'entrÃ©e CNN
        multi_device_tensor = processor.create_multi_device_tensor()
        
        # CrÃ©er le volume 3D pour VoxNet
        volume_3d = processor.create_3d_volume()
        
        logger.info("âœ… Traitement des donnÃ©es terminÃ© avec succÃ¨s")
        logger.info(f"Forme du tenseur multi-dispositifs: {multi_device_tensor.shape}")
        logger.info(f"Forme du volume 3D: {volume_3d.shape}")
                
        return processor, multi_device_tensor, volume_3d
    
    # ============================================================================
    # PHASE 3: PRÃ‰PARATION DES DONNÃ‰ES D'ENTRAÃNEMENT
    # ============================================================================
    
    def phase3_data_preparation(processor: Any, multi_device_tensor: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Phase 3: Division et prÃ©paration des donnÃ©es pour l'entraÃ®nement.
        
        Args:
            processor: Processeur de donnÃ©es
            multi_device_tensor: Tenseur multi-dispositifs
            
        Returns:
            Tuple contenant (x_train, x_test)
        """
        logger.info("\nğŸ”€ Phase 3: Division et prÃ©paration des donnÃ©es")
        logger.info("-" * 40)
                
        if multi_device_tensor is None:
            logger.warning("Aucun tenseur multi-dispositifs disponible")
            return np.array([]), np.array([])
        
        # Diviser les donnÃ©es pour l'entraÃ®nement
        x_train, x_test = processor.split_data(multi_device_tensor)
        
        logger.info(f"Taille de l'ensemble d'entraÃ®nement: {len(x_train)}")
        logger.info(f"Taille de l'ensemble de test: {len(x_test)}")
            
        return x_train, x_test
    
    # ============================================================================
    # PHASE 4: ENTRAÃNEMENT DES MODÃˆLES
    # ============================================================================
    
    def phase4_model_training(model_type: str = "cnn_2d", 
                             x_train: Optional[np.ndarray] = None,
                             x_test: Optional[np.ndarray] = None,
                             volume_3d: Optional[np.ndarray] = None,
                             training_config: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Phase 4: EntraÃ®nement des modÃ¨les CNN.
        
        Args:
            model_type: Type de modÃ¨le ("cnn_2d", "cnn_3d", "hybrid", "dataframe")
            x_train: DonnÃ©es d'entraÃ®nement 2D
            x_test: DonnÃ©es de test 2D
            volume_3d: Volume 3D pour VoxNet
            training_config: Configuration d'entraÃ®nement
            
        Returns:
            Dict contenant les rÃ©sultats d'entraÃ®nement
        """
        logger.info(f"\nğŸ¤– Phase 4: EntraÃ®nement du modÃ¨le {model_type.upper()}")
        logger.info("-" * 40)
            
        # Configuration par dÃ©faut
        if training_config is None:
            training_config = {
                "epochs": 100,
                "learning_rate": 0.001,
                "batch_size": 32,
                "patience": 10,
                "device": "auto"
            }
        
        # Initialiser l'augmenteur de donnÃ©es
        from src.preprocessor.data_augmenter import GeophysicalDataAugmenter
        augmenter = GeophysicalDataAugmenter()
        
        # Initialiser l'entraÃ®neur
        from src.model.geophysical_trainer import GeophysicalTrainer
        trainer = GeophysicalTrainer(augmenter, device=training_config["device"])
        
        results = {}
        
        try:
            if model_type == "cnn_2d":
                results = train_cnn_2d(trainer, x_train, x_test, training_config)
            elif model_type == "cnn_3d":
                results = train_cnn_3d(trainer, volume_3d, training_config)
            elif model_type == "hybrid":
                results = train_hybrid_model(trainer, training_config)
            elif model_type == "dataframe":
                results = train_dataframe_model(trainer, training_config)
            else:
                raise ValueError(f"Type de modÃ¨le non supportÃ©: {model_type}")
            
            logger.info("âœ… EntraÃ®nement terminÃ© avec succÃ¨s")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'entraÃ®nement: {str(e)}")
            raise
    
    def train_cnn_2d(trainer: Any, x_train: np.ndarray, x_test: np.ndarray, config: Dict) -> Dict[str, Any]:
        """EntraÃ®ner un modÃ¨le CNN 2D."""
        from src.model.geophysical_trainer import GeophysicalCNN2D
        
        logger.info("EntraÃ®nement du modÃ¨le CNN 2D...")
        
        # CrÃ©er des labels factices pour la dÃ©monstration
        # Dans un vrai projet, vous auriez des vrais labels
        y_train = np.random.randint(0, 2, len(x_train))
        y_test = np.random.randint(0, 2, len(x_test))
        
        # CrÃ©er le modÃ¨le
        model = GeophysicalCNN2D(
            input_channels=4,
            num_classes=2,
            grid_size=64,
            dropout_rate=0.3
        )
        
        # PrÃ©parer les donnÃ©es - convertir de 4D Ã  3D pour chaque Ã©chantillon
        x_train_3d = []
        for i in range(len(x_train)):
            # Convertir de (channels, height, width) Ã  (height, width, channels)
            sample = np.transpose(x_train[i], (1, 2, 0))
            x_train_3d.append(sample)
        
        x_test_3d = []
        for i in range(len(x_test)):
            sample = np.transpose(x_test[i], (1, 2, 0))
            x_test_3d.append(sample)
        
        # Combiner train et test pour la prÃ©paration
        all_data = x_train_3d + x_test_3d
        all_labels = y_train.tolist() + y_test.tolist()
        
        # PrÃ©parer les donnÃ©es
        train_loader, val_loader = trainer.prepare_data_2d(
            all_data, all_labels,
            augmentations=["rotation", "flip_horizontal", "gaussian_noise"],
            num_augmentations=3,
            test_size=0.2
        )
        
        # EntraÃ®ner le modÃ¨le
        history = trainer.train_model(
            model, train_loader, val_loader,
            num_epochs=config["epochs"],
            learning_rate=config["learning_rate"],
            patience=config["patience"]
        )
        
        # Sauvegarder le modÃ¨le
        model_path = CONFIG.paths.artifacts_dir / "models" / "cnn_2d_model.pth"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        trainer.save_model(model, str(model_path))
        
        return {
            "model_type": "CNN_2D",
            "model": model,
            "history": history,
            "model_path": str(model_path)
        }
    
    def train_cnn_3d(trainer: Any, volume_3d: np.ndarray, config: Dict) -> Dict[str, Any]:
        """EntraÃ®ner un modÃ¨le CNN 3D."""
        from src.model.geophysical_trainer import GeophysicalCNN3D
        
        logger.info("EntraÃ®nement du modÃ¨le CNN 3D...")
        
        if volume_3d is None:
            logger.warning("Aucun volume 3D disponible, crÃ©ation d'un volume factice")
            volume_3d = np.random.rand(10, 4, 32, 32, 32)
        
        # CrÃ©er des labels factices
        y_labels = np.random.randint(0, 2, len(volume_3d))
        
        # CrÃ©er le modÃ¨le
        model = GeophysicalCNN3D(
            input_channels=4,
            num_classes=2,
            volume_size=32,
            dropout_rate=0.3
        )
        
        # PrÃ©parer les donnÃ©es
        train_loader, val_loader = trainer.prepare_data_3d(
            volume_3d.tolist(), y_labels.tolist(),
            augmentations=["rotation", "gaussian_noise"],
            num_augmentations=2,
            test_size=0.2
        )
        
        # EntraÃ®ner le modÃ¨le
        history = trainer.train_model(
            model, train_loader, val_loader,
            num_epochs=config["epochs"],
            learning_rate=config["learning_rate"],
            patience=config["patience"]
        )
        
        # Sauvegarder le modÃ¨le
        model_path = CONFIG.paths.artifacts_dir / "models" / "cnn_3d_model.pth"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        trainer.save_model(model, str(model_path))
        
        return {
            "model_type": "CNN_3D",
            "model": model,
            "history": history,
            "model_path": str(model_path)
        }
    
    def train_hybrid_model(trainer: Any, config: Dict) -> Dict[str, Any]:
        """EntraÃ®ner un modÃ¨le hybride (images + donnÃ©es gÃ©ophysiques)."""
        from src.model.geophysical_image_trainer import GeophysicalImageTrainer
        
        logger.info("EntraÃ®nement du modÃ¨le hybride...")
        
        # CrÃ©er le trainer hybride
        hybrid_trainer = GeophysicalImageTrainer(trainer.augmenter, device=config["device"])
        
        # Pour la dÃ©monstration, utiliser des chemins d'images factices
        # Dans un vrai projet, vous auriez de vrais chemins d'images
        image_paths = [
            str(CONFIG.paths.raw_data_dir / "images" / "resistivity" / "resis1.JPG"),
            str(CONFIG.paths.raw_data_dir / "images" / "resistivity" / "resis2.JPG"),
            str(CONFIG.paths.raw_data_dir / "images" / "chargeability" / "char_1.PNG"),
            str(CONFIG.paths.raw_data_dir / "images" / "chargeability" / "char_2.PNG"),
        ]
        
        # CrÃ©er des donnÃ©es gÃ©ophysiques factices
        geo_data = [np.random.rand(10).tolist() for _ in range(len(image_paths))]
        labels = np.random.randint(0, 2, len(image_paths)).tolist()
        
        # CrÃ©er le modÃ¨le hybride
        from src.model.geophysical_hybrid_net import GeophysicalHybridNet
        model = GeophysicalHybridNet(num_classes=2)
        
        # PrÃ©parer les donnÃ©es hybrides
        train_loader, val_loader = hybrid_trainer.prepare_hybrid_data(
            image_paths, geo_data, labels,
            test_size=0.2,
            augmentations=["rotation", "brightness"],
            num_augmentations=2
        )
        
        # EntraÃ®ner le modÃ¨le
        history = hybrid_trainer.train_hybrid_model(
            model, train_loader, val_loader,
            num_epochs=config["epochs"],
            learning_rate=config["learning_rate"],
            patience=config["patience"]
        )
        
        # Sauvegarder le modÃ¨le
        model_path = CONFIG.paths.artifacts_dir / "models" / "hybrid_model.pth"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        torch.save(model.state_dict(), str(model_path))
        
        return {
            "model_type": "HYBRID",
            "model": model,
            "history": history,
            "model_path": str(model_path)
        }
    
    def train_dataframe_model(trainer: Any, config: Dict) -> Dict[str, Any]:
        """EntraÃ®ner un modÃ¨le pour les DataFrames gÃ©ophysiques."""
        from src.model.geophysical_trainer import GeophysicalDataFrameNet
        import pandas as pd
        
        logger.info("EntraÃ®nement du modÃ¨le DataFrame...")
        
        # CrÃ©er des DataFrames factices pour la dÃ©monstration
        dataframes = []
        labels = []
        
        for i in range(20):
            # CrÃ©er un DataFrame factice avec des colonnes gÃ©ophysiques
            df = pd.DataFrame({
                'x': np.random.rand(50),
                'y': np.random.rand(50),
                'z': np.random.rand(50),
                'resistivity': np.random.uniform(1e-8, 1e9, 50),
                'chargeability': np.random.uniform(0, 200, 50)
            })
            dataframes.append(df)
            labels.append(np.random.randint(0, 2))
        
        # CrÃ©er le modÃ¨le
        model = GeophysicalDataFrameNet(
            input_features=5,  # x, y, z, resistivity, chargeability
            num_classes=2,
            hidden_layers=[256, 128, 64],
            dropout_rate=0.3
        )
        
        # PrÃ©parer les donnÃ©es
        train_loader, val_loader = trainer.prepare_data_dataframe(
            dataframes, labels,
            augmentations=["gaussian_noise", "value_variation"],
            num_augmentations=3,
            test_size=0.2
        )
        
        # EntraÃ®ner le modÃ¨le
        history = trainer.train_model(
            model, train_loader, val_loader,
            num_epochs=config["epochs"],
            learning_rate=config["learning_rate"],
            patience=config["patience"]
        )
        
        # Sauvegarder le modÃ¨le
        model_path = CONFIG.paths.artifacts_dir / "models" / "dataframe_model.pth"
        model_path.parent.mkdir(parents=True, exist_ok=True)
        trainer.save_model(model, str(model_path))
        
        return {
            "model_type": "DATAFRAME",
            "model": model,
            "history": history,
            "model_path": str(model_path)
        }
    
    # ============================================================================
    # PHASE 5: Ã‰VALUATION ET RÃ‰SULTATS
    # ============================================================================
    
    def phase5_evaluation_and_results(training_results: Dict[str, Any], 
                                    processor: Optional[Any] = None) -> Dict[str, Any]:
        """
        Phase 5: Ã‰valuation des modÃ¨les et gÃ©nÃ©ration des rÃ©sultats.
        
        Args:
            training_results: RÃ©sultats d'entraÃ®nement
            processor: Processeur de donnÃ©es
            
        Returns:
            Dict contenant les rÃ©sultats d'Ã©valuation
        """
        logger.info("\nğŸ“ˆ Phase 5: Ã‰valuation et rÃ©sultats")
        logger.info("-" * 40)
            
        results = {
            "training_results": training_results,
            "evaluation_metrics": {},
            "model_summary": {}
        }
        
        # GÃ©nÃ©rer le rÃ©sumÃ© des donnÃ©es si disponible
        if processor and hasattr(processor, 'get_data_summary'):
                try:
                    data_summary = processor.get_data_summary()
                    results["data_summary"] = data_summary
                    
                    # Sauvegarder le rÃ©sumÃ© dans les artefacts
                    artifacts_dir = Path(CONFIG.paths.artifacts_dir)
                    artifacts_dir.mkdir(parents=True, exist_ok=True)
                    
                    summary_file = artifacts_dir / "training_summary.json"
                    with open(summary_file, 'w') as f:
                     json.dump(results, f, indent=2, default=str)
                    
                    logger.info(f"RÃ©sumÃ© d'entraÃ®nement sauvegardÃ© dans: {summary_file}")
                except Exception as e:
                    logger.warning(f"Impossible de gÃ©nÃ©rer le rÃ©sumÃ© des donnÃ©es: {e}")
        
        # GÃ©nÃ©rer le rÃ©sumÃ© du modÃ¨le
        if training_results:
            model_type = training_results.get("model_type", "UNKNOWN")
            model_path = training_results.get("model_path", "N/A")
            history = training_results.get("history", {})
            
            results["model_summary"] = {
                "model_type": model_type,
                "model_path": model_path,
                "total_epochs": len(history.get("epochs", [])),
                "final_train_loss": history.get("train_loss", [])[-1] if history.get("train_loss") else "N/A",
                "final_val_loss": history.get("val_loss", [])[-1] if history.get("val_loss") else "N/A",
                "final_train_acc": history.get("train_accuracy", [])[-1] if history.get("train_accuracy") else "N/A",
                "final_val_acc": history.get("val_accuracy", [])[-1] if history.get("val_accuracy") else "N/A"
            }
            
            logger.info(f"RÃ©sumÃ© du modÃ¨le {model_type}:")
            logger.info(f"  - Chemin: {model_path}")
            logger.info(f"  - Ã‰poques: {results['model_summary']['total_epochs']}")
            train_loss = results['model_summary']['final_train_loss']
            val_loss = results['model_summary']['final_val_loss']
            train_acc = results['model_summary']['final_train_acc']
            val_acc = results['model_summary']['final_val_acc']
            
            if isinstance(train_loss, (int, float)) and isinstance(val_loss, (int, float)):
                logger.info(f"  - Loss finale (train/val): {train_loss:.4f}/{val_loss:.4f}")
            else:
                logger.info(f"  - Loss finale (train/val): {train_loss}/{val_loss}")
                
            if isinstance(train_acc, (int, float)) and isinstance(val_acc, (int, float)):
                logger.info(f"  - Accuracy finale (train/val): {train_acc:.2f}%/{val_acc:.2f}%")
            else:
                logger.info(f"  - Accuracy finale (train/val): {train_acc}%/{val_acc}%")
        
        return results
    
    # ============================================================================
    # FONCTION PRINCIPALE
    # ============================================================================
    
    def main():
        """
        Pipeline principal pour le projet AI-MAP.
        """
        try:
            logger.info("ğŸš€ Starting AI-MAP Pipeline")
            logger.info("=" * 60)
            
            # Phase 1: Nettoyage des donnÃ©es
            cleaning_results = phase1_data_cleaning()
            
            # Phase 2: Traitement des donnÃ©es
            processor, multi_device_tensor, volume_3d = phase2_data_processing()
            
            # Phase 3: PrÃ©paration des donnÃ©es
            x_train, x_test = phase3_data_preparation(processor, multi_device_tensor)
            
            # Phase 4: EntraÃ®nement (sÃ©lection du modÃ¨le via arguments)
            training_config = {
                "epochs": 50,  # RÃ©duit pour la dÃ©monstration
                "learning_rate": 0.001,
                "batch_size": 32,
                "patience": 10,
                "device": "auto"
            }
            
            # EntraÃ®ner le modÃ¨le sÃ©lectionnÃ©
            training_results = phase4_model_training(
                model_type="cnn_2d",  # Par dÃ©faut, peut Ãªtre changÃ© via CLI
                x_train=x_train,
                x_test=x_test,
                volume_3d=volume_3d,
                training_config=training_config
            )
            
            # Phase 5: Ã‰valuation et rÃ©sultats
            final_results = phase5_evaluation_and_results(training_results, processor)
            
            # Statut final
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ‰ PIPELINE AI-MAP TERMINÃ‰ AVEC SUCCÃˆS!")
            logger.info("=" * 60)
            
            logger.info("ğŸ“‹ Ce qui a Ã©tÃ© accompli:")
            logger.info("  âœ… Nettoyage et validation des donnÃ©es")
            logger.info("  âœ… Transformation et alignement des coordonnÃ©es")
            logger.info("  âœ… CrÃ©ation des grilles spatiales")
            logger.info("  âœ… Normalisation des donnÃ©es")
            logger.info("  âœ… PrÃ©paration du tenseur multi-dispositifs")
            logger.info("  âœ… CrÃ©ation du volume 3D")
            logger.info("  âœ… Division des donnÃ©es d'entraÃ®nement/test")
            logger.info("  âœ… EntraÃ®nement du modÃ¨le sÃ©lectionnÃ©")
            logger.info("  âœ… Ã‰valuation et sauvegarde des rÃ©sultats")
            
            logger.info(f"\nğŸš€ ModÃ¨le entraÃ®nÃ©: {training_results.get('model_type', 'UNKNOWN')}")
            logger.info(f"ğŸ“ ModÃ¨le sauvegardÃ©: {training_results.get('model_path', 'N/A')}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Le pipeline a Ã©chouÃ©: {str(e)}")
            logger.error("Trace de la pile:", exc_info=True)
            raise
    
    # ============================================================================
    # INTERFACE EN LIGNE DE COMMANDE
    # ============================================================================
    
    def parse_arguments():
        """Parser les arguments de la ligne de commande."""
        parser = argparse.ArgumentParser(
            description="Pipeline AI-MAP pour l'entraÃ®nement de modÃ¨les gÃ©ophysiques",
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
Exemples d'utilisation:
  python main.py --model cnn_2d --epochs 100
  python main.py --model hybrid --epochs 50 --learning-rate 0.0001
  python main.py --model cnn_3d --batch-size 16 --patience 15
  python main.py --model dataframe --epochs 200 --device cuda
            """
        )
        
        # SÃ©lection du modÃ¨le
        parser.add_argument(
            "--model", "-m",
            choices=["cnn_2d", "cnn_3d", "hybrid", "dataframe"],
            default="cnn_2d",
            help="Type de modÃ¨le Ã  entraÃ®ner (dÃ©faut: cnn_2d)"
        )
        
        # ParamÃ¨tres d'entraÃ®nement
        parser.add_argument(
            "--epochs", "-e",
            type=int,
            default=50,
            help="Nombre d'Ã©poques d'entraÃ®nement (dÃ©faut: 50)"
        )
        
        parser.add_argument(
            "--learning-rate", "-lr",
            type=float,
            default=0.001,
            help="Taux d'apprentissage (dÃ©faut: 0.001)"
        )
        
        parser.add_argument(
            "--batch-size", "-b",
            type=int,
            default=32,
            help="Taille du batch (dÃ©faut: 32)"
        )
        
        parser.add_argument(
            "--patience", "-p",
            type=int,
            default=10,
            help="Patience pour l'early stopping (dÃ©faut: 10)"
        )
        
        parser.add_argument(
            "--device", "-d",
            choices=["auto", "cpu", "cuda"],
            default="auto",
            help="Device pour l'entraÃ®nement (dÃ©faut: auto)"
        )
        
        # Options de pipeline
        parser.add_argument(
            "--skip-cleaning",
            action="store_true",
            help="Passer la phase de nettoyage des donnÃ©es"
        )
        
        parser.add_argument(
            "--skip-processing",
            action="store_true",
            help="Passer la phase de traitement des donnÃ©es"
        )
        
        parser.add_argument(
            "--skip-training",
            action="store_true",
            help="Passer la phase d'entraÃ®nement (utile pour tester le pipeline)"
        )
        
        # Options de sortie
        parser.add_argument(
            "--output-dir", "-o",
            type=str,
            default=None,
            help="RÃ©pertoire de sortie pour les modÃ¨les (dÃ©faut: artifacts/models/)"
        )
        
        parser.add_argument(
            "--verbose", "-v",
            action="store_true",
            help="Mode verbeux pour plus de dÃ©tails"
        )
        
        return parser.parse_args()
    
    def main_with_args():
        """Fonction main avec gestion des arguments CLI."""
        args = parse_arguments()
        
        # Configuration du logging
        if args.verbose:
            import logging
            logging.getLogger().setLevel(logging.DEBUG)
        
        # Configuration d'entraÃ®nement
        training_config = {
            "epochs": args.epochs,
            "learning_rate": args.learning_rate,
            "batch_size": args.batch_size,
            "patience": args.patience,
            "device": args.device
        }
        
        # RÃ©pertoire de sortie
        if args.output_dir:
            output_path = Path(args.output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            CONFIG.paths.artifacts_dir = output_path
        
        logger.info("ğŸš€ Starting AI-MAP Pipeline with CLI arguments")
        logger.info("=" * 60)
        logger.info(f"Configuration:")
        logger.info(f"  - ModÃ¨le: {args.model}")
        logger.info(f"  - Ã‰poques: {args.epochs}")
        logger.info(f"  - Learning rate: {args.learning_rate}")
        logger.info(f"  - Batch size: {args.batch_size}")
        logger.info(f"  - Patience: {args.patience}")
        logger.info(f"  - Device: {args.device}")
        logger.info(f"  - Output: {CONFIG.paths.artifacts_dir}")
        logger.info("=" * 60)
        
        try:
            # Phase 1: Nettoyage des donnÃ©es
            if not args.skip_cleaning:
                cleaning_results = phase1_data_cleaning()
            else:
                logger.info("â­ï¸  Phase 1: Nettoyage des donnÃ©es (ignorÃ©e)")
                cleaning_results = {}
            
            # Phase 2: Traitement des donnÃ©es
            if not args.skip_processing:
                processor, multi_device_tensor, volume_3d = phase2_data_processing()
            else:
                logger.info("â­ï¸  Phase 2: Traitement des donnÃ©es (ignorÃ©e)")
                processor, multi_device_tensor, volume_3d = None, None, None
            
            # Phase 3: PrÃ©paration des donnÃ©es
            if not args.skip_processing:
                x_train, x_test = phase3_data_preparation(processor, multi_device_tensor)
            else:
                logger.info("â­ï¸  Phase 3: PrÃ©paration des donnÃ©es (ignorÃ©e)")
                x_train, x_test = np.array([]), np.array([])
            
            # Phase 4: EntraÃ®nement
            if not args.skip_training:
                training_results = phase4_model_training(
                    model_type=args.model,
                    x_train=x_train,
                    x_test=x_test,
                    volume_3d=volume_3d,
                    training_config=training_config
                )
            else:
                logger.info("â­ï¸  Phase 4: EntraÃ®nement (ignorÃ©e)")
                training_results = {"model_type": args.model, "model_path": "N/A"}
            
            # Phase 5: Ã‰valuation et rÃ©sultats
            final_results = phase5_evaluation_and_results(training_results, processor)
            
            # Statut final
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ‰ PIPELINE AI-MAP TERMINÃ‰ AVEC SUCCÃˆS!")
            logger.info("=" * 60)
            
            logger.info("ğŸ“‹ RÃ©sumÃ© de l'exÃ©cution:")
            logger.info(f"  âœ… ModÃ¨le entraÃ®nÃ©: {args.model}")
            logger.info(f"  âœ… Ã‰poques: {args.epochs}")
            logger.info(f"  âœ… Device: {args.device}")
            if training_results.get("model_path"):
                logger.info(f"  âœ… ModÃ¨le sauvegardÃ©: {training_results['model_path']}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Le pipeline a Ã©chouÃ©: {str(e)}")
            logger.error("Trace de la pile:", exc_info=True)
            raise
    
    # ============================================================================
    # POINT D'ENTRÃ‰E
    # ============================================================================
    
    if __name__ == "__main__":
        # VÃ©rifier si des arguments sont fournis
        if len(sys.argv) > 1:
            # Mode CLI avec arguments
            success = main_with_args()
        else:
            # Mode par dÃ©faut (sans arguments)
            success = main()
        
        if success:
            logger.info("ğŸ¯ Pipeline terminÃ© avec succÃ¨s!")
        else:
            logger.error("ğŸ’¥ Le pipeline a Ã©chouÃ©!")
            sys.exit(1)
            
except ImportError as e:
    print(f"âŒ Erreur d'import: {e}")
    print("Veuillez vÃ©rifier que tous les packages requis sont installÃ©s:")
    print("pip install -r requirements.txt")
    sys.exit(1)
    
except Exception as e:
    print(f"âŒ Erreur inattendue: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
